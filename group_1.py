# -*- coding: utf-8 -*-
"""GROUP 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mfl2uErEyf7RsBGZF4svOx0xsFW5d91D
"""

from google.colab import drive
import pandas as pd

# Mount Google Drive
drive.mount('/content/drive')

# Define file path
file_path = "/content/drive/MyDrive/AD PROJECT/GROUP 1 DATASET - Sheet1.csv"

# Load dataset
df = pd.read_csv(file_path)
df.head()

from sklearn.preprocessing import OneHotEncoder

# One-hot encode Crop and Season
encoder = OneHotEncoder(sparse_output=False, drop="first")  # Fix: Changed 'sparse' to 'sparse_output'
encoded_categorical = encoder.fit_transform(df_processed[["Crop", "Season"]])

# Convert encoded data to a DataFrame
encoded_columns = encoder.get_feature_names_out(["Crop", "Season"])
df_encoded = pd.DataFrame(encoded_categorical, columns=encoded_columns)

# Show encoded values for a few rows
print("\nOne-Hot Encoded Sample:\n", df_encoded.head())

# Drop original categorical columns
df_numerical = df_processed.drop(columns=["Crop", "Season"])

# Merge encoded categorical data
df_final = pd.concat([df_numerical, df_encoded], axis=1)

# Show final dataset structure
print("\nFinal Processed Data (First 5 Rows):\n", df_final.head())
print("\nColumns in Final Data:\n", df_final.columns)

# Define features (X) and target (Y)
X = df_final.drop(columns=["Yield", "Severity_Value"])  # Excluding Yield & Severity as targets
Y = df_final["Yield"]  # Predicting Yield

# Show feature sample
print("\nFeatures Sample:\n", X.head())
print("\nTarget Sample:\n", Y.head())

from sklearn.preprocessing import StandardScaler

# Standardize numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Convert back to DataFrame for better visualization
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

# Show a small sample after scaling
print("\nScaled Features Sample:\n", X_scaled_df.head())

from sklearn.model_selection import train_test_split

# Split into training (80%) and testing (20%) sets
X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)

# Show the number of samples in each set
print("\nTraining Samples:", X_train.shape[0])
print("Testing Samples:", X_test.shape[0])

# Verify the first few samples
print("\nX_train Sample:\n", X_train[:5])
print("\nY_train Sample:\n", Y_train[:5])

import pandas as pd

# Save the training and testing sets to CSV files
X_train_df = pd.DataFrame(X_train)
X_test_df = pd.DataFrame(X_test)
Y_train_df = pd.DataFrame(Y_train)
Y_test_df = pd.DataFrame(Y_test)

# Specify the file paths where you'd like to save them
X_train_df.to_csv('/content/drive/MyDrive/AD PROJECT/X_train.csv', index=False)
X_test_df.to_csv('/content/drive/MyDrive/AD PROJECT/X_test.csv', index=False)
Y_train_df.to_csv('/content/drive/MyDrive/AD PROJECT/Y_train.csv', index=False)
Y_test_df.to_csv('/content/drive/MyDrive/AD PROJECT/Y_test.csv', index=False)

# Display the saved files' sample (first few rows)
print("\nSample of X_train:\n", X_train_df.head(5))
print("\nSample of Y_train:\n", Y_train_df.head(5))

# Import necessary libraries
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam

# Load your dataset (assuming you've loaded it correctly)
# For example, X_scaled and Y are your features and target variables
# X_scaled is your scaled feature data, Y is your target variable

# Split into training (80%) and testing (20%) sets
X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)

# Reshape X_train and X_test for LSTM (3D: [samples, timesteps, features])
X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

# Build LSTM Model
lstm_model = Sequential()
lstm_model.add(LSTM(64, activation='relu', input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))
lstm_model.add(Dense(1))
lstm_model.compile(optimizer=Adam(), loss='mean_squared_error')

# Train the LSTM Model
lstm_model.fit(X_train_lstm, Y_train, epochs=10, batch_size=32, verbose=1)

# Make predictions using LSTM
lstm_predictions = lstm_model.predict(X_test_lstm)

# Evaluate LSTM
lstm_mae = mean_absolute_error(Y_test, lstm_predictions)
lstm_rmse = np.sqrt(mean_squared_error(Y_test, lstm_predictions))
print("LSTM MAE:", lstm_mae)
print("LSTM RMSE:", lstm_rmse)

# Now, build and train a Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train Random Forest
rf_model.fit(X_train, Y_train)

# Make predictions using Random Forest
rf_predictions = rf_model.predict(X_test)

# Evaluate Random Forest
rf_mae = mean_absolute_error(Y_test, rf_predictions)
rf_rmse = np.sqrt(mean_squared_error(Y_test, rf_predictions))
print("Random Forest MAE:", rf_mae)
print("Random Forest RMSE:", rf_rmse)

from sklearn.metrics import r2_score

# Calculate R² for both models
lstm_r2 = r2_score(Y_test, lstm_predictions)
rf_r2 = r2_score(Y_test, rf_predictions)

# Calculate Accuracy in percentage for both models (Relative MAE)
lstm_accuracy_percent = (1 - (lstm_mae / Y_test.mean())) * 100
rf_accuracy_percent = (1 - (rf_mae / Y_test.mean())) * 100

# Print the results
print("LSTM R²: ", lstm_r2)
print("Random Forest R²: ", rf_r2)

print("\nLSTM Accuracy (in %): ", lstm_accuracy_percent)
print("Random Forest Accuracy (in %): ", rf_accuracy_percent)

# Calculate Mean Absolute Percentage Error (MAPE) for LSTM
lstm_mape = np.mean(np.abs((Y_test.values.flatten() - lstm_predictions) / Y_test.values.flatten())) * 100

# Calculate Mean Absolute Percentage Error (MAPE) for Random Forest
rf_mape = np.mean(np.abs((Y_test.values.flatten() - rf_predictions) / Y_test.values.flatten())) * 100

# Print results
print("LSTM MAPE (in %):", lstm_mape)
print("Random Forest MAPE (in %):", rf_mape)

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import seaborn as sns

# Step 1: Evaluation (Assuming you already have `lstm_predictions` and `rf_predictions`)

# Evaluate LSTM
lstm_mae = mean_absolute_error(Y_test, lstm_predictions)
lstm_rmse = np.sqrt(mean_squared_error(Y_test, lstm_predictions))
lstm_r2 = r2_score(Y_test, lstm_predictions)
lstm_accuracy_percent = (1 - (lstm_mae / Y_test.mean())) * 100
lstm_mape = np.mean(np.abs((Y_test.values.flatten() - lstm_predictions) / Y_test.values.flatten())) * 100

# Evaluate Random Forest
rf_mae = mean_absolute_error(Y_test, rf_predictions)
rf_rmse = np.sqrt(mean_squared_error(Y_test, rf_predictions))
rf_r2 = r2_score(Y_test, rf_predictions)
rf_accuracy_percent = (1 - (rf_mae / Y_test.mean())) * 100
rf_mape = np.mean(np.abs((Y_test.values.flatten() - rf_predictions) / Y_test.values.flatten())) * 100

# Print all the evaluation results
print("LSTM MAE:", lstm_mae)
print("LSTM RMSE:", lstm_rmse)
print("LSTM R²:", lstm_r2)
print("LSTM Accuracy (in %):", lstm_accuracy_percent)
print("LSTM MAPE (in %):", lstm_mape)

print("Random Forest MAE:", rf_mae)
print("Random Forest RMSE:", rf_rmse)
print("Random Forest R²:", rf_r2)
print("Random Forest Accuracy (in %):", rf_accuracy_percent)
print("Random Forest MAPE (in %):", rf_mape)



# Step 2: Visualizing Metrics - MAE, RMSE, R², Accuracy, and MAPE
metrics = ['MAE', 'RMSE', 'R²', 'Accuracy (%)', 'MAPE (%)']
lstm_values = [lstm_mae, lstm_rmse, lstm_r2, lstm_accuracy_percent, lstm_mape]
rf_values = [rf_mae, rf_rmse, rf_r2, rf_accuracy_percent, rf_mape]

fig, ax = plt.subplots(2, 2, figsize=(15, 12))

# Plotting MAE, RMSE, R², Accuracy, and MAPE for LSTM and Random Forest
ax[0, 0].bar(metrics, lstm_values, color='blue', alpha=0.7, label='LSTM')
ax[0, 0].set_title('LSTM Metrics')
ax[0, 0].set_ylabel('Value')

ax[0, 1].bar(metrics, rf_values, color='green', alpha=0.7, label='Random Forest')
ax[0, 1].set_title('Random Forest Metrics')
ax[0, 1].set_ylabel('Value')

# Comparative Metrics
ax[1, 0].bar(metrics, lstm_values, width=0.4, color='blue', align='center', label='LSTM')
ax[1, 0].bar(metrics, rf_values, width=0.4, color='green', align='edge', label='Random Forest')
ax[1, 0].set_title('Comparison of LSTM vs Random Forest')
ax[1, 0].set_ylabel('Value')

# Step 3: Visualizing Predicted vs Actual Values for Regression Models

# Scatter Plot for LSTM predictions vs actual values
ax[1, 1].scatter(Y_test, lstm_predictions, color='blue', alpha=0.5)
ax[1, 1].plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], color='red', lw=2, linestyle='--')
ax[1, 1].set_title('LSTM: Predicted vs Actual')
ax[1, 1].set_xlabel('Actual Values')
ax[1, 1].set_ylabel('Predicted Values')

# Remove empty plots
fig.delaxes(ax[1, 1])

plt.tight_layout()
plt.show()

# Display first 10 sample predictions for both LSTM and Random Forest models
import pandas as pd

# Combine true values with predictions for both models
results_df = pd.DataFrame({
    'True Values': Y_test.values.flatten(),
    'LSTM Predictions': lstm_predictions.flatten(),
    'Random Forest Predictions': rf_predictions.flatten()
})

# Display the first 10 samples of true values and predictions
print("Sample Predictions:")
print(results_df.head(10))